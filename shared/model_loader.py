from pathlib import Path
import requests
from typing import Optional, List, Dict, Any, Tuple
import logging
from shared.config import MODELS_DIR, MODEL_CONFIG, PROCESSING_CONFIG, GPU_AVAILABLE
from transformers import AutoProcessor, AutoModelForVision2Seq
from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor
import torch
# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logger = logging.getLogger(__name__)


class ModelLoader:
    def __init__(self):
        self.device = torch.device(MODEL_CONFIG["device"])
        self.models_dir = MODELS_DIR
        self.models_dir.mkdir(exist_ok=True)
        self.loaded_models = {}
        self.model_cache = {}

        print(f"ğŸ¯ Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {self.device}")
        if GPU_AVAILABLE:
            print(f"ğŸ¯ Ø°Ø§ÙƒØ±Ø© GPU Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.2f} GB")

    def _download_model(self, model_url: str, model_path: Path) -> bool:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† URL"""
        try:
            print(f"ğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù†: {model_url}")

            response = requests.get(model_url, stream=True, timeout=MODEL_CONFIG["model_download_timeout"])
            response.raise_for_status()

            total_size = int(response.headers.get('content-length', 0))
            downloaded = 0

            with open(model_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        if total_size > 0:
                            progress = (downloaded / total_size) * 100
                            print(f"ğŸ“Š Ø§Ù„ØªÙ‚Ø¯Ù…: {progress:.1f}%", end='\r')

            print(f"\nâœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­: {model_path.name}")
            return True

        except Exception as e:
            print(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
            if model_path.exists():
                model_path.unlink()
            return False

    def _setup_model_cache(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„Ù†Ù…Ø§Ø°Ø¬"""
        if MODEL_CONFIG["optimize_for_inference"]:
            torch.backends.cudnn.benchmark = True
            if GPU_AVAILABLE:
                torch.backends.cuda.matmul.allow_tf32 = True
                torch.backends.cudnn.allow_tf32 = True



    def load_qwen2_vl_model(self, model_name: str = "Qwen/Qwen2-VL-2B-Instruct"):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Qwen2-VL Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙˆØµÙ"""
        cache_key = f"qwen2_vl_{model_name}"
        if cache_key in self.model_cache:
            logger.info(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ Qwen2-VL ({model_name}) ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©.")
            return self.model_cache[cache_key]
        try:
            logger.info(f"ğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Qwen2-VL: {model_name}")
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… AutoProcessor Ùˆ AutoModelForVision2Seq
            self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
            model = Qwen2VLForConditionalGeneration.from_pretrained("Qwen/Qwen2-VL-2B-Instruct", torch_dtype="auto")
            model.to( self.device )
            # default processer
            processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")
            self.model_cache[cache_key] = (processor, model)
            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Qwen2-VL: {model_name} Ø¨Ù†Ø¬Ø§Ø­ Ø¹Ù„Ù‰ {self.device}")
            return processor, model
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Qwen2-VL: {e}")
            return None, None


    def clear_model_cache(self, model_key: str = None):
        """Ù…Ø³Ø­ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„Ù†Ù…Ø§Ø°Ø¬"""
        if model_key:
            if model_key in self.model_cache:
                # ØªÙØ±ÙŠØº Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                model_instance = self.model_cache[model_key]
                if isinstance(model_instance, tuple): # Ø¥Ø°Ø§ ÙƒØ§Ù† tuple (Ù…Ø«Ù„ BLIP: processor, model)
                    for item in model_instance:
                        del item
                else:
                    del model_instance
                del self.model_cache[model_key]
                logger.info(f"ğŸ§¹ ØªÙ… Ù…Ø³Ø­ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {model_key}")
        else:
            # Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            for key in list(self.model_cache.keys()):
                self.clear_model_cache(key) # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø© Ù†ÙØ³Ù‡Ø§ Ù„ØªÙØ±ÙŠØº ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø­Ø¯Ø©
            if GPU_AVAILABLE:
                torch.cuda.empty_cache()
            logger.info("ğŸ§¹ ØªÙ… Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©")

    def get_model_memory_usage(self) -> Dict[str, float]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„Ù„Ù†Ù…Ø§Ø°Ø¬"""
        memory_usage = {}

        for model_name, model in self.model_cache.items():
            try:
                if hasattr(model, 'parameters'):
                    # Ø­Ø³Ø§Ø¨ Ø­Ø¬Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
                    param_size = sum(p.numel() * p.element_size() for p in model.parameters())
                    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())
                    total_size = (param_size + buffer_size) / 1024 ** 2  # MB

                    memory_usage[model_name] = total_size
            except:
                pass

        return memory_usage

    def load_scene_recognition_model(self):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Qwen2-VL ÙÙ‚Ø· Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ù‡Ø¯/Ø§Ù„Ù†Ø´Ø§Ø·."""
        qwen2_vl_processor, qwen2_vl_model = self.load_qwen2_vl_model()
        # Ù†ÙØ±Ø¬Ø¹ Ù‚ÙŠÙ…ØªÙŠÙ† ÙÙ‚Ø· Ø§Ù„Ø¢Ù†
        return qwen2_vl_processor, qwen2_vl_model

    def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        self.model_cache.clear()
        if GPU_AVAILABLE:
            torch.cuda.empty_cache()
        print("ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø¬Ù…ÙŠØ¹ Ù…ÙˆØ§Ø±Ø¯ ModelLoader")



# Ø¥Ù†Ø´Ø§Ø¡ loader Ø¹Ø§Ù„Ù…ÙŠ
model_loader = ModelLoader()

def cleanup_models():
    """ØªÙ†Ø¸ÙŠÙ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
    model_loader.cleanup()

